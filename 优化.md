# 1. HDFS核心参数
## 1.1 NameNode内存生产配置
1. 每个文件约占150byte,128Gb可以存9.1亿个文件块
2. Hadoop2.x默认内存2000m,HADOOP_NAMENODE_OPTS=-Xmx3072m
3. Hadoop3.x,hadoop-env.sh中描述内存是基于机器动态分配的
4. 查看NameNode占用内存,jsp显示进程号和名,jmap -heap -ID
5. 默认NameNode和DataNode的MaxHeapSize=984MB
6. 经验参考值,namenode最小1G,每增加100万个block,增加1G内存
7. 经验参考值,datanode最小4G,block或副本数高于400万个时,每增加100万,增加1G
```
//hadoop-env.sh
export HDFS_NAMENODE_OPTS="-Dhadoop.security.logger=INFO,RFAS -Xmx1024m"
export HDFS_DATANODE_OPTS="-Dhadoop.security.logger=ERROR,RFAS -Xmx1024m"
```
## 1.2 NameNode心跳并发配置
1. NameNode中有个工作线程池,用来处理处理不同的DataNode的并发心跳和客户端并发元数据操作
2. hdfs-site.xml,dfs.name.handler.count=20*loge(clusterSize),集群3台 20\*loge3=21 

## 1.3 开启回收站
1. fs.trash.interval=0,文件存活时间,0表示禁用,不为零启用
2. fs.trash.checkpoint.interval=0,回收站检查间隔,0表示和(fs.trash.interval)文件存活时间相同
3. fs.trash.checkpoint.interval<=fs.trash.interval
4. 回收站目录/user/z/.Trash
5. 网页上删除不走回收站
6. 程序中删除需要new Trash(conf).moveToTrash(path);
7. 命令行走回收站hadoop fs -rm -r path 
8. 恢复hadoop fs -mv /user/z/.Trash/Current/user/z/path /path

# 2. HDFS-集群压测

### 2.1 集群写性能
1. 向集群写10个文件每个128M,文件数=集群CPU核数-1
```
 hadoop jar /opt/module/hadoop3.1.3/share/hadoop/mapreduce/adoop-mapreduce-clientjobclient-3.1.3-tests.jar TestDFSIO -write -nrFiles 10 -fileSize 128MB
//Throughput=总数据量/总时间
//Average IO rate =(map1平均速度+...+mapn平均速度)/n
//IO rate std deviation 偏差,各个mapTask处理的差值
//一共参与测试文件10*2个副本,一个副本在本地,客户端不在集群则算3个副本
//压测后速度1.61,实际速度1.61*20=32M/s
//三台服务器带宽每台100mbps,12.5*3=30m/s
```
2. 测试出现异常,yarn-site.xml,设置虚拟内存检测false,分发到集群并重启Yarn集群
```
<!--是否启动一个线程检查每个任务正使用的虚拟内存量，如果任务超出分配值，则
直接将其杀掉，默认是 true -->
<property>
	<name>yarn.nodemanager.vmem-check-enabled</name>
	<value>false</value>
</property>
```
### 2.2 集群读性能
```
hadoop jar /opt/module/hadoop3.1.3/share/hadoop/mapreduce/adoop-mapreduce-clientjobclient-3.1.3-tests.jar TestDFSIO -read -nrFiles 10 -fileSize 128MB
```
# 3. HDFS多目录
### 3.1 NameNode多目录,多个name目录,内容相同,增加可靠性
```
//hdfs-site.xml,每台机器配置可不同
//??需要删除data和logs目录并重新格式化namenode??
//hdfs namenode -format
<property>
	<name>dfs.namenode.name.dir</name>
<value>file://${hadoop.tmp.dir}/dfs/name1,file://${hadoop.tmp.
dir}/dfs/name2</value>
</property>
```
### 3.2 DataNode多目录,内容不同
```
<property>
   <name>dfs.datanode.data.dir</name>
   <value>file://${hadoop.tmp.dir}/dfs/data1,file://${hadoop.tmp.dir}/dfs/data2</value>
</property>
```
### 3.3 磁盘间的数据均衡,Hadoop3.x新特性,机器硬盘空间不足,增加磁盘时
1. 生成均衡计划
2. 执行均衡任务
3. 查看均衡任务执行情况
4. 取消均衡任务
```
hdfs diskbalancer -plan hadoop102
hdfs diskbalancer -excute hadoop102.plan.json
hdfs diskbalancer -query hadoop102
hdfs diskbalancer -cancel hadoop102.plan.json
```
# 4. HDFS 集群扩容及缩容
### 4.1 白名单和黑名单
1. 配置hdfs-site.xml
2. 分发hdfs-site.xml文件
3. 第一次配置hdfs-site.xml黑白名单,需要重启集群
4. 修改whitelist和blacklist只需要刷新集群hdfs dfsadmin -refreshNodes
```
//在hdfs-site.xml中配置dfs.hosts参数
<property>
   <name>dfs.hosts</name>
   <value>/opt/module/hadoop-3.1.3/etc/hadoop/whitelist</value>
</property>
//黑名单
<property>
   <name>dfs.hosts.exclude</name>
   <value>/opt/module/hadoop-3.1.3/etc/hadoop/blacklist</value>
</property>
```
### 4.2 服役新的服务器
1. 环境准备,克隆新机器
2. 修改IP地址和主机名称
3. 白名单添加新机器,xsync whitelist
4. 拷贝java和hadoop环境,和环境变量/etc/profile.d/my_env.sh 
5. 删除新机器中data和log数据
6. 配置namenode和resourcemanager机器的ssh免密登录,ssh -copy-id hadoop100
7. 直接启动DataNode和nodemanager, hdfs --daemon start datanode ,yarn --daemon start nodemanager
```
scp -r module/* user@hadoop100:/opt/module
sudo scp /etc/profile.d/my_env.sh root@hadoop100:/etc/profiled/my_env.sh
source /etc/profile
rm -rf data logs
ssh -copy-id hadoop100
hdfs --daemon start datanode
yarn --daemon start nodemanager
```
### 4.3 服务器间的数据均衡 
1. 根据数据本地性原则,经常在同一节点提交任务,会导致这一节点的数据量过大,其他节点数据量小的情况
2. 数据均衡命令 sbin/start-balancer.sh -threshold 10
3. threshold 阈值10,表示各个节点磁盘利用率相差不超过10%
4. 停止数据均衡sbin/stop-balancer.sh
5. 启动rebalance需要额外Rebalance server,尽量不要在namenode和resourcemanager指定start-balancer.sh
### 4.4 黑名单退役
1. blacklist中添加需要退役的节点,xsync blacklist
2. 非第一次修改hdfs-site.xml添加blacklist,只需刷新namenode节点,hdfs dfsadmin -refreshNodes
3. ip:9870/查看节点,退役节点状态为decommission in progress
4. 等待节点状态为decommissioned(所有块已经复制完成),如果副本数3,服役节点少于或等于3,无法退役成功,需要修改副本数
5. 退役后数据不均衡,可执行start-balancer.sh

# 5. HDFS存储优化
### 5.1 纠删码
1. 引入纠删码,利用算力将副本多余的开销减少
2. 命令 hdfs ec [Command](-listPolicies,-addPolicies -policyFile \<file\>)...
```
 [-listPolicies]//查看当前纠删码策略
 [-addPolicies -policyFile <file>]
 [-getPolicy -path <path>]
 [-removePolicy -policy <policy>]
 [-setPolicy -path <path> [-policy <policy>] [-replicate]]
 [-unsetPolicy -path <path>]
 [-listCodecs]
 [-enablePolicy -policy <policy>]
 [-disablePolicy -policy <policy>]
 [-help <command-name>]
```
3. 纠删码类型(编码方式-数据单元个数-校验单元个数-单元大小),
4. 任意数据单元个数的单元存在(只是个数,与是否数据单元无关),就可以得到原始数据
5. 默认开启RS-6-3-1024k
6. RS-10-4-1024k,RS-6-3-1024k,RS-LEGACY-6-3-1024k,XOR-2-1-1024k
7. 纠删码策略是给具体的一个路径
```
//开启RS-3-2-1024k
hdfs -ec -enablePolicy -policy RS-3-2-1024k
hdfs -ec -setPolicy -path path -policy RS-3-2-1024k (-replicate 2)
//（低于 2M，只有一个数据单元和两个校验单元)
```
### 5.2 异构存储(冷热数据分离)
1. 类型说明

类型  |  说明
---|---
RAN_DISK |  内存镜像文件系统
SSD   |  SSD固态硬盘
DISK  |  普通硬盘,默认
ARCHIVE  |  没有特指哪种介质,一般用作归档,指计算弱存储密度高的存储介质

1. 存储策略

策略ID | 策略名称 |副本分布
---|---|---
15 |  Lazy_Persist   |  RAM_DISK:1,DISK:n-1
12 | ALL_SSD   |  SSD:n
10 | One_SSD   |  SSD:1,DISK:n-1
7  |  Hot(default)   |  DISK:n
5  |  Warm  |  DISK:1,ARCHIVE:n-1
2  |  Cold  |  ARCHIVE:n

3. 操作
```
hdfs storagepolicies -listPolicies
//为目录设置指定策略
hdfs storagepolicies -setStoragePolicy -path <path> -policy <policy>
hdfs storagepolicies -getStoragePolicy -path <path>
hdfs storagepolicies -unsetStoragePolicy -path <path>
//查看文件块的分布
hdfs fsck <filename> -files -blocks -locations
//查看集群节点
hadoop dfsadmin report
```
4. 配置文件
```
//默认开启
<property>
   <name>dfs.storage.policy.enabled</name>
   <value>true</value>
</property>
//配置文件夹
<property>
   <name>dfs.datanode.data.dir</name>
   <value>[SSD]file:///opt/module/hadoop3.1.3/hdfsdata/ssd,[RAM_DISK]file:///opt/module/hadoop3.1.3/hdfsdata/ram_disk>[ARCHIVE]file:///opt/module/hadoop3.1.3/hdfsdata/archive</value>
</property>
```
5. 更改文件策略
```
//更改维lazy_persist
hdfs storagepolicies -setStoragePolicy-path /hdfsdata -policy lazy_persist
//手动移动文件块
hdfs mover /hdfsdata
//查看文件块分布
hdfs fsck /hdfsdata -files -blocks -locations
```
6. LAZY_PERSIST问题
   - 存储在内存中有限制
   - 默认不在内存存储数据转为DISK
   - 客户端所在DataNode节点没有RAM_DISK时,则写入客户端所在节点的DISK,其余写在其他节点的DISK
   - 客户端所在节点有RAM_DISK时,“dfs.datanode.max.locked.memory未设置或小于dfs.blocksize(128MB,134,217,728),则同上
   - 虚拟机的"max locked memory"为64kb, ulimit -a查询该数据

# 6. 故障
## 6.1 NameNode数据丢失
1. 关闭NameNode进程 kill -9 id
2. 删除数据rm -rf data/dfs/name
3. 从SecondaryNameNode拷贝数据scp -r z@hadoop100:/opt/.../namesecondary/* ./name/
4. 重启NameNode ,hdfs --daemon start namenode
## 6.2 集群安全模式
1. 安全模式,只读,无法进行删除和修改操作
2. 进入安全模式条件
   - NameNode加载镜像文件和编辑日志的时候
   - NameNode在接收DataNode注册时
3. 退出安全模式条件
   - dfs.namenode.safemode.min.datanodes:最小可用datanode数量,默认0
   - dfs.namenode.safemode.threshold-pct:副本最小要求的block站系统总block数的百分比,默认0.999f(只允许丢一块)
   - dfs.namenode.safenode.extension:稳定时间默认3000毫秒,即30秒
4. 命令
```
hdfs dfsadmin -safemode get
hdfs dfsadmin -safemode enter
hdfs dfsadmin -safemode leave
hdfs dfsadmin -safemode wait
```
5. 数据块损坏进入安全模式
   - 若数据重要,则停机修复硬盘
   - 数据不重要,离开安全模式后删除元数据,http://hadoop100:9870/dfshealth.html#tab-overview
   - 
## 6.3 慢磁盘监控
### 发现慢磁盘
- 偶尔创建文件夹时间过长
- 通过心跳未联系时间
- fio命令,测试磁盘读写性能

```
sudo yum install -y fio
//顺序测试读
sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=read -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r

Run status group 0 (all jobs):
   READ: bw=216MiB/s (226MB/s), 216MiB/s-216MiB/s (226MB/s-226MB/s), io=12.6GiB (13.6GB), run=60001-60001msec
//磁盘总体顺序读速度为216mb

//测试顺序写
sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=write -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_w

//随机写
sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randwrite -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_randw

//混合随机读写
 sudo fio -filename=/home/atguigu/test.log -direct=1 -iodepth 1 -thread -rw=randrw -rwmixread=70 -ioengine=psync -bs=16k -size=2G -numjobs=10 -runtime=60 -group_reporting -name=test_r_w -ioscheduler=noop
```
## 6.3 小文件归档
1. 小文件的弊端,大量小文件会占用NameNode的大部分内存
2. 解决小文件存储方法一,归档存储
```
//将input下所有文件归档为input.har文件,输出到/output
hadoop archive -archiveName input.har -p /input /output
//查看信息,har://
hadoop fs -ls har:///output/input.har
//解除归档,
hadoop fs -cp har:///output/input.har/* /
hadoop fs -cp har:///output/input.har/a.txt /
```
# 7.集群迁移
1. apache和apache集群间数据拷贝 
   - scp拷贝
   - hadoop distcp hdfs://hadoop100:8020/user/z/hello.txt hdfs://hadoop101:8020/user/z/hello.txt
2. apache和CDH集群间数据拷贝

# 8.MapReduce
### 8.1 MapReduce慢的原因
1. 计算机性能
2. I/O操作优化
   1. 数据倾斜
   2. Map阶段运行时间过长,导致Reduce等待太久
   3. 小文件过多
   
### 8.2 MapReduce调优参数
#### shuffle阶段优化
1. 自定义分区减少数据倾斜,继承Partitioner接口,重写getPartition方法
2. 减少溢写次数
   1. mapreduce.task.io.sort.mb shuffle环形缓冲区大小默认100m可以提高到200m
   2. mapreduce.map.sort.spill.percent 溢写阈值默认80%,可以提高到90%
3. 增加每次Merge合并次数
   1. mapreduce.task.io.sort.factor 默认10,可提高到20
4. 不影响业务的情况提前采用Combiner
   1. job.setCombinerClass(xxxReducer.class)
5. 减少磁盘I/O,采用snappy或者LZO压缩
   1. conf.setBoolean("mapreduce.map.output.compress",true)
   2. conf.setClass("mapreduce.map.output.compress.codec",SnappyCodec.class,CompressionCodec.class)
6. 提高maptask内存上限,128m数据对应1G来调
   1. mapreduce.map.memory.mb
7. maptask堆内存大小,(内存不足:java.lang.OutOfMemoryError)
   1. mapreduce.map.java.opts
8. 增加cpu核数mapreduce.map.vcores,默认1
9. 异常重试次数默认4,适当提高,mapreduce.map.maxattempts

#### Reduce阶段优化
1. 每个Reduce去map拉取数据的并行数,默认5,可提高到10
   1. mapreduce.reduce.shuffle.parallelcopies
2. Buffer大小占Reduce可用内存的比例0.7可调至0.8
   1. mapreduce.reduce.shuffle.input.buffer.percent
3. Buffer中的数据多少时写入磁盘默认0.66可提高到0.75
   1. mapreduce.reduce.shuffle.merge.percent
4. ReduceTask内存上限,128m对应1G,可调高到4-6g
   1. mapreduce.reduce.memory.mb
5. 控制Reduce的堆内存大小(内存不足:java.lang.OutOfMemoryError)
   1. mapreduce.reduce.java.opts
6. Reduce的Cpu核数默认1个可调至2-4个
   1. mapreduce.reduce.cpu.vcores
7. ReduceTask最大重试次数默认4
   1. mapreduce.reduce.maxattempts
8. MapTask完成多少为ReduceTask分配资源默认0.05
   1. mapreduce.job.reduce.slowstart.completedmaps
9. 超时时间,默认600000(10分钟)
   1.  mapreduce.task.timeout
10. 若允许尽量不用Reduce
    
### 8.3 数据倾斜
1. 数据倾斜现象
   1. 数据频率倾斜,某一区域数据量远远大于其他区域
   2. 数据大小倾斜,部分记录的大小远远大于平均值
2. 优化数据倾斜
   1. 检查是否空值过多,可自定义分区,空值+随机数打散后二次聚合
   2. 能在map阶段处理,则用Combiner,MapJoin
   3. 设置多个reduce个数

# 9. Yarn调优
### 1. 调优参数

#### Resourcemanager 相关
1. ResourceManager 处理调度器请求的线程数量
   1. yarn.resourcemanager.scheduler.client.thread-count 
2. 配置调度器
   1. yarn.resourcemanager.scheduler.class
#### Nodemanager 相关
1. NodeManager 使用内存数
   1. yarn.nodemanager.resource.memory-mb
2. NodeManager 为系统保留多少内存，和上一个参数二者取一即可
   1. yarn.nodemanager.resource.system-reserved-memory-mb
3. NodeManager 使用 CPU 核数
   1. yarn.nodemanager.resource.cpu-vcores
4. 是否将虚拟核数当作 CPU 核数,虚拟核数和物理核数乘数，例如：4 核 8 线程，该参数就应设为 2
   1. yarn.nodemanager.resource.count-logical-processors-as-cores
5. 是否让 yarn 自己检测硬件进行配置
   1. yarn.nodemanager.resource.detect-hardware-capabilities
6. 是否开启物理内存检查限制 container
   1. yarn.nodemanager.pmem-check-enabled
7. 是否开启虚拟内存检查限制 containe
   1. yarn.nodemanager.vmem-check-enabledr
8. 虚拟内存物理内存比例
   1. yarn.nodemanager.vmem-pmem-ratio

#### Container容器相关
1. 容器最小内存
   1. yarn.scheduler.minimum-allocation-mb
2. 容器最大内存
   1. yarn.scheduler.maximum-allocation-mb 
3. 容器最小核数
   1. yarn.scheduler.minimum-allocation-vcores
4. 容器最大核数
   1. yarn.scheduler.maximum-allocation-vcores 

### 2.容器调度器
### 3.公平调度器

# 10.综合优化
1. 小文件
   1. 数据采集方面,小文件先合成后再传到HDFS
   2. 存储方面,将小文件打包成一个har文件
   3. 计算方面,用CombineTextInputFormat将多个小文件生成一个单独切片
   4. 计算方面,开启uber,实现jvm重用,若Task任务计算量小开启jvm时间更长,则可以让多个Task任务运行在同一个jvm中
```
mapred-site.xml
<!-- 开启 uber 模式，默认关闭 -->
<property>
	<name>mapreduce.job.ubertask.enable</name>
	<value>true</value>
</property>
<!-- uber 模式中最大的 mapTask 数量，可向下修改 -->
<property>
	<name>mapreduce.job.ubertask.maxmaps</name>
	<value>9</value>
</property>
<!-- uber 模式中最大的 reduce 数量，可向下修改 -->
<property>
	<name>mapreduce.job.ubertask.maxreduces</name>
	<value>1</value>
</property>
<!-- uber 模式中最大的输入数据量，默认使用 dfs.blocksize 的值，可向下修
改 -->
<property>
	<name>mapreduce.job.ubertask.maxbytes</name>
	<value></value>
</property
```
## 11.测试MapReduce计算性能
#### 一个虚拟机不超过 150G 磁盘尽量不要执行这段代码
```
//使用 RandomWriter 来产生随机数，每个节点运行 10 个 Map 任务，每个 Map 产生大约 1G 大小的二进制随机数
hadoop jar /opt/module/hadoop3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples3.1.3.jar randomwriter random-data
//执行 Sort 程序
hadoop jar /opt/module/hadoop3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-examples3.1.3.jar sort random-data sorted-data
//验证数据是否真正排好序了
hadoop jar /opt/module/hadoop3.1.3/share/hadoop/mapreduce/hadoop-mapreduce-clientjobclient-3.1.3-tests.jar testmapredsort -sortInput random-data -sortOutput sorted-data
```